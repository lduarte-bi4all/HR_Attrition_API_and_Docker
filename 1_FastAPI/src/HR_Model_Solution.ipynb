{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ce290ea",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fb1636",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_curve, plot_roc_curve, classification_report, plot_confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from warnings import simplefilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12c2611",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To ignore possible unnecessary warnings\n",
    "simplefilter(action='ignore')\n",
    "pd.set_option('Display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ed14e6",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3d87de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the path to your cleaned data\n",
    "df = pd.read_csv('../../../ml-usecase-classification-humanresourcesattrition/data/HR_cleaned.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06ee8e2",
   "metadata": {},
   "source": [
    "# Split into train and test\n",
    "First of all, split into features and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae919e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['Attrition'])\n",
    "y = df['Attrition']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8db8e4f",
   "metadata": {},
   "source": [
    "For this purpose you can use `train_test_split()` or `StratifiedShuffleSplit()`. The main advantage of `StratifiedShuffleSplit()` is your train and test sets will have the same ratio of negative and positive cases. We will use and compare both. We will choose 70% of the dataset for the training set and the remaining 30% of the dataset for the testing set.\n",
    "___\n",
    "- `train_test_split()`: On this dataset, using this method, the training set will have a greater proportion of positive cases than the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875cb3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad15f5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()/len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728f0c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()/len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7f902d",
   "metadata": {},
   "source": [
    "___\n",
    "- `StratifiedShuffleSplit()`: Both training set and testing set have the same proportion of positive cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6455b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state= 2)\n",
    "for train_index, test_index in split.split(df, df['Attrition']):\n",
    "    strat_train_set = df.loc[train_index]\n",
    "    strat_test_set = df.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c4231b",
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_train_set['Attrition'].value_counts()/len(strat_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04004ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_test_set['Attrition'].value_counts()/len(strat_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9c9bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = strat_train_set.drop(columns=['Attrition'])\n",
    "X_test = strat_test_set.drop(columns = ['Attrition'])\n",
    "y_train = strat_train_set['Attrition']\n",
    "y_test = strat_test_set['Attrition']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ceef937",
   "metadata": {},
   "source": [
    "# Scale the Data\n",
    "You might need to scale your features to avoid some bias. You have many choices of scalers. In that case, the scaler used is `MinMaxScaler()` from *Scikit-Learn*. You can check all available scalers on *Scikit Learn* on [this link](https://scikit-learn.org/stable/modules/classes.html?highlight=preprocessing#module-sklearn.preprocessing). Search for `MinMaxScaler()`, `StandardScaler()` or `Robust Scaler` and try to understand which of them you should use in each case. If you want to have a visual demonstration of each scale to choose each one you think is the most suitable, please check [this link](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16185438",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = ['Age', 'DailyRate','DistanceFromHome','HourlyRate','MonthlyIncome','MonthlyRate','NumCompaniesWorked',\n",
    "                    'PercentSalaryHike','TotalWorkingYears','YearsAtCompany','YearsInCurrentRole','YearsSinceLastPromotion',\n",
    "                    'YearsWithCurrManager','WorkLifeBalance','TrainingTimesLastYear']\n",
    "\n",
    "minmax = MinMaxScaler()\n",
    "X_train_scaled = pd.DataFrame(minmax.fit_transform(X_train[numeric_features]))\n",
    "X_test_scaled = pd.DataFrame(minmax.fit_transform(X_test[numeric_features]))\n",
    "\n",
    "X_train_scaled.columns = X_train[numeric_features].columns\n",
    "X_test_scaled.columns = X_test[numeric_features].columns\n",
    "\n",
    "X_train_scaled = pd.concat([X_train_scaled.reset_index(), X_train.drop(columns=numeric_features).reset_index()], axis=1).drop(columns = ['index'])\n",
    "X_test_scaled = pd.concat([X_test_scaled.reset_index(), X_test.drop(columns=numeric_features).reset_index()], axis=1).drop(columns = ['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f9ebb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f333fa6e",
   "metadata": {},
   "source": [
    "As you could see at EDA(*Exploratory Data Analysis*) stage, *Attrition* column, the label column, is imbalanced and this is a problem to classification model. To solve this, you can use a technique called SMOTE(Synthetic Minority Oversampling Technique). You can reade more about SMOTE on [this link](https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/). Note that you can only use SMOTE on **training set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84593845",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b63597",
   "metadata": {},
   "outputs": [],
   "source": [
    "oversample = SMOTE()\n",
    "X_train_sampled, y_train_sampled = oversample.fit_resample(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417939e4",
   "metadata": {},
   "source": [
    "There are some models that have an argument which balances the data automatically. If you are using `LogisticRegression()`from *Scikit-Learn*, the argument is called *class_weights* and you just have to set it as *'balanced'*. Check the documentation of `LogisticRegression()` on [this link](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) to understand this argument. Depending on the model you are using, the name of the argument can change. However, the effect is the same. Check the documentation of each model to see how the argument is called and how you can use it to balance your data. If you use these arguments in your model to balance your data, you don't need to use SMOTE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453790a8",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da980aa",
   "metadata": {},
   "source": [
    "To do the modeling, we are choosing LogisticRegression. We should try another models and see which model performs better. We will use a function called `GridSearchCV()`: This function allows to optimize the model hyperparameters using cross validation, which means you have to choose how many folds you want to use in your validation. We recomend to use 5 folds or 10. To know what are the parameters to optimize you should consult the documentation of the model you are using. Furthermore, you should look into what are the best parameters to optimize, saving computation time. The more parameters, the longer it takes to run. This function will define the best model, according to the metrics you choose. Usually, for classification problems, the metric that is used is *roc_auc*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f057803e",
   "metadata": {},
   "source": [
    "- **Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605fb8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "\n",
    "#Define a set of hyperparameters to optimize and their values\n",
    "parameters = {'penalty':['l1','l2'], 'C': [0.001,0.01,0.1,1,10,100,1000]}\n",
    "lr_model = GridSearchCV(estimator=lr,param_grid=parameters, cv=5, scoring='roc_auc', refit=True)\n",
    "\n",
    "#Train the model\n",
    "lr_model.fit(X_train_sampled, y_train_sampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02f5bd5",
   "metadata": {},
   "source": [
    "You can see the best estimator hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382c8dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36987645",
   "metadata": {},
   "source": [
    "You can see the results of each set of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f92246",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(lr_model.cv_results_).sort_values(by='rank_test_score').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b701c5dc",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81944d7",
   "metadata": {},
   "source": [
    "To evaluate the model you should use the test set. You can use `lr_model`object to predict because `GridSearchCV()` function has an argument called *refit*. When this argument is set as **True**, `lr_model` is refitted with the best set of hyperparameters.\n",
    "\n",
    "- **Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4e26e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds= lr_model.predict(X_test_scaled)\n",
    "y_preds_proba = lr_model.predict_proba(X_test_scaled)\n",
    "preds = pd.concat([pd.Series(y_preds), pd.DataFrame(y_preds_proba)], axis=1)\n",
    "preds.columns = ['y_preds','y_pred_proba_0','y_pred_proba_1']\n",
    "preds.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba39c30",
   "metadata": {},
   "source": [
    "___\n",
    "- **Metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f4186b",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, threshold = precision_recall_curve(y_test, y_preds_proba[:,1])\n",
    "df_metrics = pd.concat([pd.DataFrame(precision), pd.DataFrame(recall), pd.DataFrame(threshold)], axis=1)\n",
    "df_metrics.columns = ['Precision','Recall','Threshold']\n",
    "df_metrics['f1'] = 2* ((df_metrics.Precision * df_metrics.Recall)/(df_metrics.Precision + df_metrics.Recall))\n",
    "df_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8edf513",
   "metadata": {},
   "source": [
    "You can plot the roc curve and check the AUC value for test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178d0ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(lr_model, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504dd2a4",
   "metadata": {},
   "source": [
    "You can compare the results with training test. If the values are too diferent (like 0.78 for test and 0.96 for train) it means that the model is overfitting and we have to solve that. One way is to do a better optimization of the hyperparameters. Another way is to implement another models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c38d7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(lr_model, X_train_sampled, y_train_sampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a86d5c4",
   "metadata": {},
   "source": [
    "You can check the classification report provided by Scikit Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf3956c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, preds['y_preds']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89786656",
   "metadata": {},
   "source": [
    "You can check the confusion matrix to see what your model is failing the most in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b549b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(lr_model, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7953771",
   "metadata": {},
   "source": [
    "The results from the previous classification report aren't optimized according to the treshold. What we can do is check the maximum *f1* value on *df_metrics* and get the correspondent threshold. Then, we take the previous computed probabilities and if the probability of class 1 is higher or equal to the threshold, we assign class 1.  Otherwise, we assign class 0. Let's do it and check the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6379b49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxf1 = df_metrics.f1.max()\n",
    "\n",
    "maxf1threshold = df_metrics[df_metrics.f1 == maxf1]['Threshold']\n",
    "\n",
    "preds.loc[preds['y_pred_proba_1']>= float(maxf1threshold), 'y_preds_t'] = 1\n",
    "preds.loc[preds['y_pred_proba_1']< float(maxf1threshold), 'y_preds_t'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ceea93",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, preds['y_preds_t']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6cdc2a",
   "metadata": {},
   "source": [
    "The results aren't better which means that the model needs to be improved. Test another models, try another combinations of hyperparameters or another aproaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8468f960",
   "metadata": {},
   "source": [
    "## After developing your model you wan't to save it. Lets do it using joblib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a0579d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbacb82",
   "metadata": {},
   "source": [
    "The next cell will create a file for your model and save it in the local working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a67a3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = './HR_API/HR_model_load.sav'\n",
    "joblib.dump(lr_model, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d660f52",
   "metadata": {},
   "source": [
    "Now you can load the model whenever you want in other files. Let's confirm creating a new variable and comparing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21874fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model_copy = joblib.load('./HR_API/HR_model_load.sav')\n",
    "y_preds_copy = lr_model_copy.predict(X_test_scaled)\n",
    "(y_preds == y_preds_copy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('Welcome_AI_ML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "999679f1eba4f7ff27d00e9f4ab787350466728b25abb3167838e11bae9278a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
